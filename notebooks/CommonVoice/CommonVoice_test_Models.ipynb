{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Models on CommonVoice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feminine voices in the training data: 263\n",
      "Masculine voices in the training data: 263\n",
      "Feminine voices in the test data: 263\n",
      "Masculine voices in the test data: 263\n"
     ]
    }
   ],
   "source": [
    "features = 'Features_F0_MFCCs'\n",
    "project_root =  os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "test_MLS = pd.read_csv(f'{project_root}/data/CommonVoice/{features}_data.csv')\n",
    "\n",
    "test_MLS_male = test_MLS[test_MLS.Gender == 1]\n",
    "test_MLS_female = test_MLS[test_MLS.Gender == 0]\n",
    "test_MLS = pd.concat([test_MLS_male[:263], test_MLS_female])\n",
    "\n",
    "print(f'Feminine voices in the training data: {len(test_MLS.Gender)- sum(test_MLS.Gender)}')\n",
    "print(f'Masculine voices in the training data: {sum(test_MLS.Gender)}')\n",
    "print(f'Feminine voices in the test data: {len(test_MLS.Gender)- sum(test_MLS.Gender)}')\n",
    "print(f'Masculine voices in the test data: {sum(test_MLS.Gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if features == 'Features':\n",
    "    X_test = test_MLS[['nobs', 'mean', 'skew', 'kurtosis', 'median', 'mode', 'std', 'low', 'peak', 'q25', 'q75', 'iqr']].copy()\n",
    "    Y_test = test_MLS[['Gender']].copy()#.values.ravel()\n",
    "elif features == 'MFCCs':\n",
    "    X_test = test_MLS[['MFCC_1', 'MFCC_2', 'MFCC_3', 'MFCC_4', 'MFCC_5', 'MFCC_6', 'MFCC_7', 'MFCC_8', 'MFCC_9', 'MFCC_10',\n",
    "                        'MFCC_11', 'MFCC_12', 'MFCC_13', 'MFCC_14', 'MFCC_15', 'MFCC_16', 'MFCC_17', 'MFCC_18', 'MFCC_19', 'MFCC_20']].copy()\n",
    "    Y_test = test_MLS[['Gender']].copy().values.ravel()\n",
    "elif features == 'Features_MFCCs':\n",
    "    X_test = test_MLS[['nobs', 'mean', 'skew', 'kurtosis', 'median', 'mode', 'std', 'low', 'peak', 'q25', 'q75', 'iqr', \n",
    "                        'MFCC_1', 'MFCC_2', 'MFCC_3', 'MFCC_4', 'MFCC_5', 'MFCC_6', 'MFCC_7', 'MFCC_8', 'MFCC_9', 'MFCC_10',\n",
    "                        'MFCC_11', 'MFCC_12', 'MFCC_13', 'MFCC_14', 'MFCC_15', 'MFCC_16', 'MFCC_17', 'MFCC_18', 'MFCC_19', 'MFCC_20']].copy()\n",
    "    Y_test = test_MLS[['Gender']].copy().values.ravel()\n",
    "elif features == 'F0':\n",
    "    X_test = test_MLS[['nobs_pitch', 'mean_pitch', 'skew_pitch', 'kurtosis_pitch', 'median_pitch', 'mode_pitch', 'std_pitch', 'low_pitch', 'peak_pitch', 'q25_pitch', 'q75_pitch', 'iqr_pitch']].copy()\n",
    "    Y_test = test_MLS[['Gender']].copy().values.ravel()\n",
    "elif features == 'F0_MFCCs':\n",
    "    X_test = test_MLS[['nobs_pitch', 'mean_pitch', 'skew_pitch', 'kurtosis_pitch', 'median_pitch', 'mode_pitch', 'std_pitch', 'low_pitch', 'peak_pitch', 'q25_pitch', 'q75_pitch', 'iqr_pitch', \n",
    "                        'MFCC_1', 'MFCC_2', 'MFCC_3', 'MFCC_4', 'MFCC_5', 'MFCC_6', 'MFCC_7', 'MFCC_8', 'MFCC_9', 'MFCC_10',\n",
    "                        'MFCC_11', 'MFCC_12', 'MFCC_13', 'MFCC_14', 'MFCC_15', 'MFCC_16', 'MFCC_17', 'MFCC_18', 'MFCC_19', 'MFCC_20']].copy()\n",
    "    Y_test = test_MLS[['Gender']].copy().values.ravel()\n",
    "elif features == 'Features_F0':\n",
    "    X_test = test_MLS[['nobs', 'mean', 'skew', 'kurtosis', 'median', 'mode', 'std', 'low', 'peak', 'q25', 'q75', 'iqr',\n",
    "        'nobs_pitch', 'mean_pitch', 'skew_pitch', 'kurtosis_pitch', 'median_pitch', 'mode_pitch', 'std_pitch', 'low_pitch', 'peak_pitch', 'q25_pitch', 'q75_pitch', 'iqr_pitch']].copy()\n",
    "    Y_test = test_MLS[['Gender']].copy().values.ravel()\n",
    "elif features == 'Features_F0_MFCCs':\n",
    "    X_test = test_MLS[['nobs', 'mean', 'skew', 'kurtosis', 'median', 'mode', 'std', 'low', 'peak', 'q25', 'q75', 'iqr',\n",
    "        'nobs_pitch', 'mean_pitch', 'skew_pitch', 'kurtosis_pitch', 'median_pitch', 'mode_pitch', 'std_pitch', 'low_pitch', 'peak_pitch', 'q25_pitch', 'q75_pitch', 'iqr_pitch', \n",
    "                        'MFCC_1', 'MFCC_2', 'MFCC_3', 'MFCC_4', 'MFCC_5', 'MFCC_6', 'MFCC_7', 'MFCC_8', 'MFCC_9', 'MFCC_10',\n",
    "                        'MFCC_11', 'MFCC_12', 'MFCC_13', 'MFCC_14', 'MFCC_15', 'MFCC_16', 'MFCC_17', 'MFCC_18', 'MFCC_19', 'MFCC_20']].copy()\n",
    "    Y_test = test_MLS[['Gender']].copy().values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "scaler = pickle.load(open(f'{project_root}/models/CETUC/{features}/scaler.pkl', 'rb'))\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree\n",
      "Accuracy on test set: 0.677\n",
      "Precision on test set:  0.672\n",
      "Recall on test set: 0.692\n",
      "F1-score on test set: 0.682\n",
      "\n",
      "Random Forests\n",
      "Accuracy on test set: 0.732\n",
      "Precision on test set:  0.670\n",
      "Recall on test set: 0.913\n",
      "F1-score on test set: 0.773\n",
      "\n",
      "Gradient Boosting\n",
      "Accuracy on test set: 0.753\n",
      "Precision on test set:  0.677\n",
      "Recall on test set: 0.966\n",
      "F1-score on test set: 0.796\n"
     ]
    }
   ],
   "source": [
    "dataset = 'CETUC'\n",
    "\n",
    "filename = f'{project_root}/models/{dataset}/{features}/DecisionTree.sav'\n",
    "tree = pickle.load(open(filename, 'rb'))\n",
    "print(\"\\nDecision Tree\")\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, Y_test)))\n",
    "cm = confusion_matrix(Y_test, tree.predict(X_test), labels=[1, 0])\n",
    "precision = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "recall = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "# print(f\"Confusion Matrix:\\n {cm}\")\n",
    "print(f\"Precision on test set:  {precision:.3f}\")\n",
    "print(f\"Recall on test set: {recall:.3f}\")\n",
    "print(f\"F1-score on test set: {(2 * (precision * recall) / (precision + recall)):.3f}\")\n",
    "\n",
    "filename = f'{project_root}/models/{dataset}/{features}/RandomForest.sav'\n",
    "forest = pickle.load(open(filename, 'rb'))\n",
    "print(\"\\nRandom Forests\")\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, Y_test)))\n",
    "cm = confusion_matrix(Y_test, forest.predict(X_test), labels=[1, 0])\n",
    "precision = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "recall = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "# print(f\"Confusion Matrix:\\n {cm}\")\n",
    "print(f\"Precision on test set:  {precision:.3f}\")\n",
    "print(f\"Recall on test set: {recall:.3f}\")\n",
    "print(f\"F1-score on test set: {(2 * (precision * recall) / (precision + recall)):.3f}\")\n",
    "\n",
    "filename = f'{project_root}/models/{dataset}/{features}/GradientBoosting.sav'\n",
    "gbrt = pickle.load(open(filename, 'rb'))\n",
    "print(\"\\nGradient Boosting\")\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, Y_test)))\n",
    "cm = confusion_matrix(Y_test, gbrt.predict(X_test), labels=[1, 0])\n",
    "precision = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "recall = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "# print(f\"Confusion Matrix:\\n {cm}\")\n",
    "print(f\"Precision on test set:  {precision:.3f}\")\n",
    "print(f\"Recall on test set: {recall:.3f}\")\n",
    "print(f\"F1-score on test set: {(2 * (precision * recall) / (precision + recall)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LogisticRegression\n",
      "Accuracy on test set: 0.804\n",
      "Precision on test set:  0.784\n",
      "Recall on test set: 0.840\n",
      "F1-score on test set: 0.811\n",
      "\n",
      "Support Vector Machine\n",
      "Accuracy on test set: 0.791\n",
      "Precision on test set:  0.752\n",
      "Recall on test set: 0.867\n",
      "F1-score on test set: 0.806\n",
      "\n",
      "Multilayer Perceptron\n",
      "Accuracy on test set: 0.728\n",
      "Precision on test set:  0.729\n",
      "Recall on test set: 0.726\n",
      "F1-score on test set: 0.728\n"
     ]
    }
   ],
   "source": [
    "filename = f'{project_root}/models/{dataset}/{features}/LogisticRegression.sav'\n",
    "lgr = pickle.load(open(filename, 'rb'))\n",
    "print(\"\\nLogisticRegression\")\n",
    "print(\"Accuracy on test set: {:.3f}\".format(lgr.score(X_test, Y_test)))\n",
    "cm = confusion_matrix(Y_test, lgr.predict(X_test), labels=[1, 0])\n",
    "precision = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "recall = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "# print(f\"Confusion Matrix:\\n {cm}\")\n",
    "print(f\"Precision on test set:  {precision:.3f}\")\n",
    "print(f\"Recall on test set: {recall:.3f}\")\n",
    "print(f\"F1-score on test set: {(2 * (precision * recall) / (precision + recall)):.3f}\")\n",
    "\n",
    "filename = f'{project_root}/models/{dataset}/{features}/SVM.sav'\n",
    "svm = pickle.load(open(filename, 'rb'))\n",
    "print(\"\\nSupport Vector Machine\")\n",
    "print(\"Accuracy on test set: {:.3f}\".format(svm.score(X_test, Y_test)))\n",
    "cm = confusion_matrix(Y_test, svm.predict(X_test), labels=[1, 0])\n",
    "precision = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "recall = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "# print(f\"Confusion Matrix:\\n {cm}\")\n",
    "print(f\"Precision on test set:  {precision:.3f}\")\n",
    "print(f\"Recall on test set: {recall:.3f}\")\n",
    "print(f\"F1-score on test set: {(2 * (precision * recall) / (precision + recall)):.3f}\")\n",
    "\n",
    "filename = f'{project_root}/models/{dataset}/{features}/MLP.sav'\n",
    "mlp = pickle.load(open(filename, 'rb'))\n",
    "print(\"\\nMultilayer Perceptron\")\n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test, Y_test)))\n",
    "cm = confusion_matrix(Y_test, mlp.predict(X_test), labels=[1, 0])\n",
    "precision = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "recall = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "# print(f\"Confusion Matrix:\\n {cm}\")\n",
    "print(f\"Precision on test set:  {precision:.3f}\")\n",
    "print(f\"Recall on test set: {recall:.3f}\")\n",
    "print(f\"F1-score on test set: {(2 * (precision * recall) / (precision + recall)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5486e5cd054d1e412d6aef716f8c2fbe82dbf0bdc56586f31f4b3a964d871afa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "236b1f587e00dfc797b67f0f09e861988e72f179bfc773f14c0ed9918c1370fd"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
