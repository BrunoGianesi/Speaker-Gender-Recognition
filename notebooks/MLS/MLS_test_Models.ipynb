{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Models on MLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 'Features_F0_MFCCs'\n",
    "dataset = 'CETUC'\n",
    "\n",
    "project_root =  os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "\n",
    "test_MLS = pd.read_csv(os.path.join(project_root, 'data', 'MLS',f'{features}_data.csv'))\n",
    "\n",
    "if features == 'Features':\n",
    "    X_test = test_MLS[['nobs', 'mean', 'skew', 'kurtosis', 'median', 'mode', 'std', 'low', 'peak', 'q25', 'q75', 'iqr']].copy()\n",
    "    Y_test = test_MLS[['Gender']].copy()#.values.ravel()\n",
    "elif features == 'MFCCs':\n",
    "    X_test = test_MLS[['MFCC_1', 'MFCC_2', 'MFCC_3', 'MFCC_4', 'MFCC_5', 'MFCC_6', 'MFCC_7', 'MFCC_8', 'MFCC_9', 'MFCC_10',\n",
    "                        'MFCC_11', 'MFCC_12', 'MFCC_13', 'MFCC_14', 'MFCC_15', 'MFCC_16', 'MFCC_17', 'MFCC_18', 'MFCC_19', 'MFCC_20']].copy()\n",
    "    Y_test = test_MLS[['Gender']].copy().values.ravel()\n",
    "elif features == 'Features_MFCCs':\n",
    "    X_test = test_MLS[['nobs', 'mean', 'skew', 'kurtosis', 'median', 'mode', 'std', 'low', 'peak', 'q25', 'q75', 'iqr', \n",
    "                        'MFCC_1', 'MFCC_2', 'MFCC_3', 'MFCC_4', 'MFCC_5', 'MFCC_6', 'MFCC_7', 'MFCC_8', 'MFCC_9', 'MFCC_10',\n",
    "                        'MFCC_11', 'MFCC_12', 'MFCC_13', 'MFCC_14', 'MFCC_15', 'MFCC_16', 'MFCC_17', 'MFCC_18', 'MFCC_19', 'MFCC_20']].copy()\n",
    "    Y_test = test_MLS[['Gender']].copy().values.ravel()\n",
    "elif features == 'F0':\n",
    "    X_test = test_MLS[['nobs_pitch', 'mean_pitch', 'skew_pitch', 'kurtosis_pitch', 'median_pitch', 'mode_pitch', 'std_pitch', 'low_pitch', 'peak_pitch', 'q25_pitch', 'q75_pitch', 'iqr_pitch']].copy()\n",
    "    Y_test = test_MLS[['Gender']].copy().values.ravel()\n",
    "elif features == 'F0_MFCCs':\n",
    "    X_test = test_MLS[['nobs_pitch', 'mean_pitch', 'skew_pitch', 'kurtosis_pitch', 'median_pitch', 'mode_pitch', 'std_pitch', 'low_pitch', 'peak_pitch', 'q25_pitch', 'q75_pitch', 'iqr_pitch', \n",
    "                        'MFCC_1', 'MFCC_2', 'MFCC_3', 'MFCC_4', 'MFCC_5', 'MFCC_6', 'MFCC_7', 'MFCC_8', 'MFCC_9', 'MFCC_10',\n",
    "                        'MFCC_11', 'MFCC_12', 'MFCC_13', 'MFCC_14', 'MFCC_15', 'MFCC_16', 'MFCC_17', 'MFCC_18', 'MFCC_19', 'MFCC_20']].copy()\n",
    "    Y_test = test_MLS[['Gender']].copy().values.ravel()\n",
    "elif features == 'Features_F0':\n",
    "    X_test = test_MLS[['nobs', 'mean', 'skew', 'kurtosis', 'median', 'mode', 'std', 'low', 'peak', 'q25', 'q75', 'iqr',\n",
    "        'nobs_pitch', 'mean_pitch', 'skew_pitch', 'kurtosis_pitch', 'median_pitch', 'mode_pitch', 'std_pitch', 'low_pitch', 'peak_pitch', 'q25_pitch', 'q75_pitch', 'iqr_pitch']].copy()\n",
    "    Y_test = test_MLS[['Gender']].copy().values.ravel()\n",
    "elif features == 'Features_F0_MFCCs':\n",
    "    X_test = test_MLS[['nobs', 'mean', 'skew', 'kurtosis', 'median', 'mode', 'std', 'low', 'peak', 'q25', 'q75', 'iqr',\n",
    "        'nobs_pitch', 'mean_pitch', 'skew_pitch', 'kurtosis_pitch', 'median_pitch', 'mode_pitch', 'std_pitch', 'low_pitch', 'peak_pitch', 'q25_pitch', 'q75_pitch', 'iqr_pitch', \n",
    "                        'MFCC_1', 'MFCC_2', 'MFCC_3', 'MFCC_4', 'MFCC_5', 'MFCC_6', 'MFCC_7', 'MFCC_8', 'MFCC_9', 'MFCC_10',\n",
    "                        'MFCC_11', 'MFCC_12', 'MFCC_13', 'MFCC_14', 'MFCC_15', 'MFCC_16', 'MFCC_17', 'MFCC_18', 'MFCC_19', 'MFCC_20']].copy()\n",
    "    Y_test = test_MLS[['Gender']].copy().values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "scaler = pickle.load(open(os.path.join(project_root, 'models', dataset, features, 'scaler.pkl'), 'rb'))\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree\n",
      "Accuracy on test set: 0.817\n",
      "Precision on test set:  0.780\n",
      "Recall on test set: 0.897\n",
      "F1-score on test set: 0.835\n",
      "\n",
      "Random Forests\n",
      "Accuracy on test set: 0.856\n",
      "Precision on test set:  0.876\n",
      "Recall on test set: 0.839\n",
      "F1-score on test set: 0.857\n",
      "\n",
      "Gradient Boosting\n",
      "Accuracy on test set: 0.915\n",
      "Precision on test set:  0.889\n",
      "Recall on test set: 0.953\n",
      "F1-score on test set: 0.920\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(project_root, 'models', dataset, features, 'DecisionTree.sav')\n",
    "tree = pickle.load(open(filename, 'rb'))\n",
    "print(\"\\nDecision Tree\")\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, Y_test)))\n",
    "cm = confusion_matrix(Y_test, tree.predict(X_test), labels=[1, 0])\n",
    "precision = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "recall = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "# print(f\"Confusion Matrix:\\n {cm}\")\n",
    "print(f\"Precision on test set:  {precision:.3f}\")\n",
    "print(f\"Recall on test set: {recall:.3f}\")\n",
    "print(f\"F1-score on test set: {(2 * (precision * recall) / (precision + recall)):.3f}\")\n",
    "\n",
    "filename = os.path.join(project_root, 'models', dataset, features, 'RandomForest.sav')\n",
    "forest = pickle.load(open(filename, 'rb'))\n",
    "print(\"\\nRandom Forests\")\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, Y_test)))\n",
    "cm = confusion_matrix(Y_test, forest.predict(X_test), labels=[1, 0])\n",
    "precision = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "recall = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "# print(f\"Confusion Matrix:\\n {cm}\")\n",
    "print(f\"Precision on test set:  {precision:.3f}\")\n",
    "print(f\"Recall on test set: {recall:.3f}\")\n",
    "print(f\"F1-score on test set: {(2 * (precision * recall) / (precision + recall)):.3f}\")\n",
    "\n",
    "filename = os.path.join(project_root, 'models', dataset, features, 'GradientBoosting.sav')\n",
    "gbrt = pickle.load(open(filename, 'rb'))\n",
    "print(\"\\nGradient Boosting\")\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, Y_test)))\n",
    "cm = confusion_matrix(Y_test, gbrt.predict(X_test), labels=[1, 0])\n",
    "precision = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "recall = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "# print(f\"Confusion Matrix:\\n {cm}\")\n",
    "print(f\"Precision on test set:  {precision:.3f}\")\n",
    "print(f\"Recall on test set: {recall:.3f}\")\n",
    "print(f\"F1-score on test set: {(2 * (precision * recall) / (precision + recall)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LogisticRegression\n",
      "Accuracy on test set: 0.908\n",
      "Precision on test set:  0.896\n",
      "Recall on test set: 0.928\n",
      "F1-score on test set: 0.912\n",
      "\n",
      "Support Vector Machine\n",
      "Accuracy on test set: 0.571\n",
      "Precision on test set:  0.546\n",
      "Recall on test set: 0.966\n",
      "F1-score on test set: 0.698\n",
      "\n",
      "Multilayer Perceptron\n",
      "Accuracy on test set: 0.784\n",
      "Precision on test set:  0.968\n",
      "Recall on test set: 0.600\n",
      "F1-score on test set: 0.740\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(project_root, 'models', dataset, features, 'LogisticRegression.sav')\n",
    "lgr = pickle.load(open(filename, 'rb'))\n",
    "print(\"\\nLogisticRegression\")\n",
    "print(\"Accuracy on test set: {:.3f}\".format(lgr.score(X_test, Y_test)))\n",
    "cm = confusion_matrix(Y_test, lgr.predict(X_test), labels=[1, 0])\n",
    "precision = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "recall = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "# print(f\"Confusion Matrix:\\n {cm}\")\n",
    "print(f\"Precision on test set:  {precision:.3f}\")\n",
    "print(f\"Recall on test set: {recall:.3f}\")\n",
    "print(f\"F1-score on test set: {(2 * (precision * recall) / (precision + recall)):.3f}\")\n",
    "\n",
    "filename = os.path.join(project_root, 'models', dataset, features, 'SVM.sav')\n",
    "svm = pickle.load(open(filename, 'rb'))\n",
    "print(\"\\nSupport Vector Machine\")\n",
    "print(\"Accuracy on test set: {:.3f}\".format(svm.score(X_test, Y_test)))\n",
    "cm = confusion_matrix(Y_test, svm.predict(X_test), labels=[1, 0])\n",
    "precision = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "recall = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "# print(f\"Confusion Matrix:\\n {cm}\")\n",
    "print(f\"Precision on test set:  {precision:.3f}\")\n",
    "print(f\"Recall on test set: {recall:.3f}\")\n",
    "print(f\"F1-score on test set: {(2 * (precision * recall) / (precision + recall)):.3f}\")\n",
    "\n",
    "filename = os.path.join(project_root, 'models', dataset, features, 'MLP.sav')\n",
    "mlp = pickle.load(open(filename, 'rb'))\n",
    "print(\"\\nMultilayer Perceptron\")\n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test, Y_test)))\n",
    "cm = confusion_matrix(Y_test, mlp.predict(X_test), labels=[1, 0])\n",
    "precision = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "recall = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "# print(f\"Confusion Matrix:\\n {cm}\")\n",
    "print(f\"Precision on test set:  {precision:.3f}\")\n",
    "print(f\"Recall on test set: {recall:.3f}\")\n",
    "print(f\"F1-score on test set: {(2 * (precision * recall) / (precision + recall)):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1*6*6*3+6*3+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5787037037037037"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(5*5*5)/(6*6*6)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5486e5cd054d1e412d6aef716f8c2fbe82dbf0bdc56586f31f4b3a964d871afa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "5416e886b2da67312ca4f5cf753d3133e2603bbf2f07750dd1ae6cf6c6d20287"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
