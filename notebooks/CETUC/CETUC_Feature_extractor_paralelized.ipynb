{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import time\n",
    "import nest_asyncio\n",
    "from speechpy import feature\n",
    "import aiofiles\n",
    "import csv\n",
    "import math\n",
    "import parselmouth\n",
    "import matplotlib.pyplot as plt\n",
    "import amfm_decompy.pYAAPT as pYAAPT"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Frequency Features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract Features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "async def sleep_sent(count):\n",
    "    print(f'Time: {time.time() - start_time:.2f} Count Sent: {count}')\n",
    "    await asyncio.sleep(0.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "async def get_frequencies(file):\n",
    "\n",
    "    file_path = os.path.join(project_root,\"CETUC\", \"Full\", file)\n",
    "\n",
    "    if file[0] == 'F':\n",
    "        gender = 0\n",
    "    if file[0] == 'M': \n",
    "        gender = 1\n",
    "    audio_data, sample_rate = librosa.load(file_path, sr=16000)\n",
    "\n",
    "    step = int(3200) #3200 sampling points every 1/5 sec\n",
    "    window_frequencies = []\n",
    "\n",
    "    for i in range(0,len(audio_data),step):\n",
    "        ft = np.fft.fft(audio_data[i:i+step]) #fft returns the list N complex numbers\n",
    "        freqs = librosa.fft_frequencies(sr=16000, n_fft=len(ft))\n",
    "        freqs = np.fft.fftfreq(len(ft)) #fftq tells you the frequencies associated with the coefficients\n",
    "        imax = np.argmax(np.abs(ft))\n",
    "        freq = freqs[imax]\n",
    "        freq_in_hz = abs(freq *sample_rate)\n",
    "        window_frequencies.append(freq_in_hz)\n",
    "\n",
    "    \n",
    "    return window_frequencies, gender, file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "async def get_features(count, file):\n",
    "    async with sem:\n",
    "        frequencies, gender, file_name = await get_frequencies(file)\n",
    "\n",
    "        nobs, minmax, mean, variance, skew, kurtosis =  stats.describe(frequencies)\n",
    "        median   = np.median(frequencies)\n",
    "        mode     = stats.mode(frequencies).mode[0]\n",
    "        std      = np.std(frequencies)\n",
    "        low,peak = minmax\n",
    "        q75,q25  = np.percentile(frequencies, [75 ,25])\n",
    "        iqr      = q75 - q25\n",
    "\n",
    "        features_list.append([file_name, nobs, mean, skew, kurtosis, median, mode, std, low, peak, q25, q75, iqr, gender])\n",
    "\n",
    "        print(f\"\\r{count}/{len(audio_files)}\", end='')\n",
    "\n",
    "        return nobs, mean, skew, kurtosis, median, mode, std, low, peak, q25, q75, iqr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# #Calculo de tempo de disparo\n",
    "start_time = time.time()\n",
    "\n",
    "#inicio do Loop\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "#Controle de requisições por vez\n",
    "sem = asyncio.Semaphore(6000)\n",
    "\n",
    "#Array de tasks\n",
    "sents = []\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#Coleta as recomendações para envio\n",
    "gender_list = []\n",
    "file_list = []\n",
    "features_list = []\n",
    "\n",
    "\n",
    "project_root =  os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "\n",
    "file_path = os.path.join(project_root,\"CETUC\", \"Full\")\n",
    "audio_files = os.listdir(file_path)\n",
    "for k, file in enumerate(audio_files):\n",
    "    sent = asyncio.ensure_future(get_features(count=k+1, file=file))\n",
    "    #sent = asyncio.create_task(get_features(count=k+1, files=audio_files))\n",
    "    \n",
    "    sents.append(sent)\n",
    " \n",
    "done, _ = loop.run_until_complete(asyncio.wait(sents))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26075/100998"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ee905043c874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0msents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tcc/Speaker-Gender-Recognition/venv/lib/python3.8/site-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_destroy_pending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tcc/Speaker-Gender-Recognition/venv/lib/python3.8/site-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cancelled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/asyncio/events.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSystemExit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tcc/Speaker-Gender-Recognition/venv/lib/python3.8/site-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(task, exc)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mcurr_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_tasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mstep_orig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurr_task\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c4b71a59162c>\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(count, file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mfrequencies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mget_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mnobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkurtosis\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-286672474563>\u001b[0m in \u001b[0;36mget_frequencies\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#fft returns the list N complex numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mfreqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mfreqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfftfreq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#fftq tells you the frequencies associated with the coefficients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mfft\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/tcc/Speaker-Gender-Recognition/venv/lib/python3.8/site-packages/numpy/fft/_pocketfft.py\u001b[0m in \u001b[0;36mfft\u001b[0;34m(a, n, axis, norm)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_unitary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0minv_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_raw_fft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tcc/Speaker-Gender-Recognition/venv/lib/python3.8/site-packages/numpy/fft/_pocketfft.py\u001b[0m in \u001b[0;36m_raw_fft\u001b[0;34m(a, n, axis, is_real, is_forward, inv_norm)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpfi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26481/100998"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save to dataframe and to .csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataframe_features = pd.DataFrame(features_list, columns = ['FileName', 'nobs', 'mean', 'skew', 'kurtosis', 'median', 'mode', 'std', 'low', 'peak', 'q25', 'q75', 'iqr', 'Gender'])\n",
    "dataframe_features.to_csv('data/CETUC/Features_data.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MFCCs "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract MFCCs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "async def extract_MFCCs(count, file):\n",
    "    async with sem:\n",
    "        file_path = os.path.join(project_root,\"CETUC\", \"Full\", file)\n",
    "        \n",
    "        audio_data, sample_rate =librosa.load(file_path)\n",
    "        \n",
    "        mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate)\n",
    "        \n",
    "        mfccs_mean = list(np.mean(mfccs.T, axis= 0))\n",
    "        if file[0] == 'F':\n",
    "            gender = 0\n",
    "        if file[0] == 'M': \n",
    "            gender = 1\n",
    "        \n",
    "        sample_features = mfccs_mean\n",
    "        sample_features.insert(0,str(file))\n",
    "        sample_features.append(gender)\n",
    "        print(f\"\\r{count}/{len(audio_files)}\",end='')\n",
    "        string = ','.join(str(item) for item in sample_features)\n",
    "        # async with aiofiles.open('CETUC_MFCCs_test.csv', mode='a') as f:\n",
    "        #     await f.write(f'\\n{string}')\n",
    "        features_list.append(sample_features)\n",
    "        \n",
    "        \n",
    "        return "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# #Calculo de tempo de disparo\n",
    "start_time = time.time()\n",
    "\n",
    "#inicio do Loop\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "#Controle de requisições por vez\n",
    "sem = asyncio.Semaphore(600)\n",
    "\n",
    "#Array de tasks\n",
    "sents = []\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#Coleta as recomendações para envio\n",
    "gender_list = []\n",
    "file_list = []\n",
    "features_list = []\n",
    "\n",
    "\n",
    "project_root =  os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "file_path = os.path.join(project_root,\"CETUC\", \"Full\")\n",
    "audio_files = os.listdir(file_path)\n",
    "\n",
    "for k, file in enumerate(audio_files):\n",
    "    sent = asyncio.ensure_future(extract_MFCCs(count=k+1, file=file))\n",
    "    \n",
    "    sents.append(sent)\n",
    " \n",
    "done, _ = loop.run_until_complete(asyncio.wait(sents))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataframe_features = pd.DataFrame(features_list, columns = ['FileName',\n",
    "                                                            'MFCC_1',  'MFCC_2',  'MFCC_3',  'MFCC_4',  'MFCC_5',\n",
    "                                                            'MFCC_6',  'MFCC_7',  'MFCC_8',  'MFCC_9',  'MFCC_10',\n",
    "                                                            'MFCC_11',  'MFCC_12',  'MFCC_13',  'MFCC_14',  'MFCC_15',\n",
    "                                                            'MFCC_16',  'MFCC_17',  'MFCC_18',  'MFCC_19',  'MFCC_20',\n",
    "                                                            'Gender'])\n",
    "dataframe_features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataframe_features.to_csv('data/CETUC/MFCCs_data.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# F0"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract F0"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "async def extract_F0(count, file):\n",
    "    async with sem:\n",
    "        file_path = os.path.join(project_root,\"CETUC\", \"Full\", file)\n",
    "        audio_data = parselmouth.Sound(file_path)\n",
    "        pitch = audio_data.to_pitch()\n",
    "        pitch_values = pitch.selected_array['frequency']\n",
    "        \n",
    "\n",
    "        nobs_pitch, minmax_pitch, mean_pitch, variance_pitch, skew_pitch, kurtosis_pitch =  stats.describe(pitch_values)\n",
    "        median_pitch   = np.median(pitch_values)\n",
    "        mode_pitch     = stats.mode(pitch_values).mode[0]\n",
    "        std_pitch      = np.std(pitch_values)\n",
    "        low_pitch,peak_pitch = minmax_pitch\n",
    "        q75_pitch,q25_pitch  = np.percentile(pitch_values, [75 ,25])\n",
    "        iqr_pitch      = q75_pitch - q25_pitch\n",
    "\n",
    "        \n",
    "        if file[0] == 'F':\n",
    "            gender = 0\n",
    "        if file[0] == 'M': \n",
    "            gender = 1\n",
    "        \n",
    "        sample_features = [nobs_pitch, mean_pitch, skew_pitch, kurtosis_pitch, median_pitch, mode_pitch, std_pitch, low_pitch, peak_pitch, q25_pitch, q75_pitch, iqr_pitch]\n",
    "        sample_features.insert(0,str(file))\n",
    "        sample_features.append(gender)\n",
    "        print(f\"\\r{count}/{len(audio_files)}\",end='')\n",
    "        string = ','.join(str(item) for item in sample_features)\n",
    "        # async with aiofiles.open('CETUC_F0_test.csv', mode='a') as f:\n",
    "        #     await f.write(f'\\n{string}')\n",
    "        features_list.append(sample_features)\n",
    "        \n",
    "\n",
    "        return "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# #Calculo de tempo de disparo\n",
    "start_time = time.time()\n",
    "\n",
    "#inicio do Loop\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "#Controle de requisições por vez\n",
    "sem = asyncio.Semaphore(600)\n",
    "\n",
    "#Array de tasks\n",
    "sents = []\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#Coleta as recomendações para envio\n",
    "gender_list = []\n",
    "file_list = []\n",
    "features_list = []\n",
    "\n",
    "\n",
    "project_root =  os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "file_path = os.path.join(project_root,\"CETUC\", \"Full\")\n",
    "audio_files = os.listdir(file_path)\n",
    "\n",
    "for k, file in enumerate(audio_files):\n",
    "    sent = asyncio.ensure_future(extract_F0(count=k+1, file=file))\n",
    "    \n",
    "    sents.append(sent)\n",
    " \n",
    "done, _ = loop.run_until_complete(asyncio.wait(sents))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataframe_features = pd.DataFrame(features_list, columns = ['FileName',\n",
    "                                                            'nobs_pitch', 'mean_pitch', 'skew_pitch', \n",
    "                                                            'kurtosis_pitch', 'median_pitch', 'mode_pitch', \n",
    "                                                            'std_pitch', 'low_pitch', 'peak_pitch', \n",
    "                                                            'q25_pitch', 'q75_pitch', 'iqr_pitch',\n",
    "                                                            'Gender'])\n",
    "dataframe_features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataframe_features.to_csv('data/CETUC/F0_data.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "5416e886b2da67312ca4f5cf753d3133e2603bbf2f07750dd1ae6cf6c6d20287"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}